<a name="br1"></a> 

**Финальная работа курса**

**“Введение в Data Science”**

Козлов Алексей Геннадьевич

специализация: Machine Learning Engine



<a name="br2"></a> 

**Анализ сайта «СберАвтоподписка».**

**Задачи:**

Предсказать факт совершения пользователем целевого

действия (оставить заявку, заказать звонок). Метрика **ROC-**

**AUC ~ 0.65**

Упаковать полученную модель предсказания в сервис,

который берет на вход данные с сайта sberautopodpiska.ru

и отдает на выход предсказание - **совершит**

**пользователь целевое действие или нет**



<a name="br3"></a> 

**1 - Разведочный анализ данных**

**Даны 2 датасета:**

GA Sessions (данные ви итов

п

GA Hits (данные действий пользователя в рамках визита)



<a name="br4"></a> 

**1.1 - Структура данных и особенности в данных**

★ Данные из двух датасетов необходимы для создания модели

★ В обоих датасетах присутствуют дубликаты

★ В обоих датасетах присутствуют пропуски в данных



<a name="br5"></a> 

**1.2 - Выводы из данных**

Необходимо:

★ Извлечь из таблицы GA Hits факт целевого действия

★ Извлечь из таблицы GA Hits необходимые данные о действиях

пользователя

★ Объединить эти данные с GA Sessions

★ Удалить дубликаты сессий, которые нам не нужны, из объединенного

датасета

★ Заполнить пропуски в данных объединенного датасета



<a name="br6"></a> 

**1.3 - Необходимые преобразование над данными**

★ Предварительная обработка данных была вынесена в отдельный модуль

dataset\_generator.py - “Генерация объединенного датасета” для

облегчения PipeLine

★ На вход поступают 2 датасета GA Sessions и GA Hits

★ На выходе функции объединенный датасет d



<a name="br7"></a> 

**2 - Data Preparation**

Данный этап реализован в модуле pipeline.py и включает в себя:

★ “new\_features” - генерацию новых фичей

★ “normal\_device\_browser” - нормализацию строковых данных

★ “normal\_screen\_resolution” - преобразование категориальных переменных

★ “remove\_hidden\_spaces” - удаление скрытых пропусков в датасете

★ “fill\_missing\_values” - преобразование пропусков в датасете

★ “encoder” - применение нестандартного кодирования данных для

экономии ресурсов ПК



<a name="br8"></a> 

**2.1 - Генерация новых фичей**

Функция “new\_features” включает в себя:

★ Подфункцию вычисления длины url - “hit\_len\_path” из столбца

“hit\_page\_path” объединенного датафрейма

★ Подфункцию “hit\_day\_hour”, которая вычисляет день недели и время

действий клиента из объединенного датафрейма

★ Подфункцию “visit\_hour”, которая вычисляет час визита “visit\_hour”

пользователя из объединенного датафрейма



<a name="br9"></a> 

**2.1.1 - Генерация новых фичей - функция “hit\_len\_path”**

Длина URL у пользователей совершивших

целевое действие и у пользователей не

совершивших целевых действий в среднем

значительно различается, поэтому было

принято решение использовать это как фича



<a name="br10"></a> 

**2.1.2 - Генерация новых фичей - функция “**“**hit\_day\_hour**”**”**

Справа приведено распределение

целевых и нецелевых действий по

дням недели. Видно, что чаще

всего

клиенты во

вторник

совершили целевое действие, а в

понедельник чаще всего не

совершали. Аналогично и по

времени, когда целевое действие

выполнялось.

Было

принято

решение использовать эти две

фичи.



<a name="br11"></a> 

**2.1.3 - Генерация новых фичей - функция “visit\_hour”**

“visit\_hour” - генерирует новую фичу “час визита” - “visit\_hour” из времени

визита объединенного датасета. Итого получается 6 новый фичей

приведенных ниже в таблице:



<a name="br12"></a> 

**2.2 - Нормализация строковых данных**

Было обнаружено, что данные в столбце “device\_browser” некорректные

до преобразования:

после функции



<a name="br13"></a> 

**2.3 - Преобразование категориальных переменных**

Количество уникальных значений в колонке device\_screen\_resolution = 4964.

Таким образом, датасет после кодировщика получит просто огромную

размерность. Функция normal\_screen\_resolution преобразовывает эти данные

в несколько этапов:

★ Подфункция “pixels” преобразует строковые данные в количество

пикселей устройства пользователя

★ Далее функция “pixels\_range” разделяет пиксели по категориям “Low”,

“Medium”, “High” и “Other”.



<a name="br14"></a> 

**2.4 - Data Cleaning**

Реализуется в двух функциях:

★ “remove\_hidden\_spaces” - удаление скрытых пропусков типа '(none)' и

'(not set)' из всего датасета

★ “fill\_missing\_values” - преобразование пропусков в датасете по принципу:

✓ Если в колонке больше 80% пропусков, то данные не иформативные и

колонка подлежит удалению

✓ Если пропусков < 20%, то пропуски заполняются модой

✓ Во всех остальных случаях, чтобы датафрейм остался цельным

пропуски заполняются “Other”



<a name="br15"></a> 

**2.5 - Нестандартное кодирование категориальных признаков**

Стандартными методами кодирования

к ат. признаков преобразовать датасет

затруднительно. Размерность после

преобразования по прежнему огромна.

Поэтому было принято решение закодировать категориальные признаки

нестандартным методом в функции “encoder\_cat”:

★ Во внешнем цикле датасет группируется по значениям каждой категориальной переменной (group\_key)

★ Во внутреннем цикле для каждой из остальных переменных (passive\_key) по категориям group\_key строятся

распределения по значениям, то есть сколько раз значения group\_key - passive\_key встречались вместе

★ Далее из этих условных распределений вычисляются их энтропии

★ На выходе функции получается объект np.array, который добавляется для обучения в модель после

StandardScaler

★ Таким образом размерность датасета уменьшается до 132



<a name="br16"></a> 

**3 - Modelling**

Моделинг реализован в модуле pipeline.py. Наилучшим образом показал себя

многоуровневый классификатор Perceptron:

**ROC-AUC ~ 0.91**

Исходя из матрицы ошибок со стандартным порогом 50%

можно сделать вывод, что 54% предсказанных значений

будут верно классифицированы как класс 1



<a name="br17"></a> 

**4 - Deployment**

Внедрение модели реализовано через сервис в модуле main.py:

★ Программа распаковывает PipeLine с моделью из файла

“sber\_auto\_pipe.pkl” с помощью модуля dill

★ Прием Get и Post запросов из внешних источников реализован через

FastAPI декораторы @app.get и @app.post

★ В зависимости от опций запроса программа выдает:

✓ "/status" - свой статус (работает она или нет)

✓ /version" - текущую версию программы

✓ "/predict" - предсказание совершит пользователь целевое действие

или н ет, обращаясь к распакованной модели



<a name="br18"></a> 

**4.1 - Deployment**

Опция "/predict" запускает функцию “predict”, которая берет на вход json файл

из тела post-запроса и через форму Form унаследованной из pydantic

преобразовывает в датафрейм df, который приходит на вход модели

Сервис был протестирован в модуле “service\_check.py”:

lient\_id’

совершит

ли нет



<a name="br19"></a> 

**5 - Заключение**

**1 Задача** - Предсказать факт совершения пользователем целевого

действия (оставить заявку, заказать звонок). Метрика ROC-AUC ~ 0.65

Выполнена: **Метрика ROC-AUC ~ 0.91**

**2 Задача** - Упаковать полученную модель предсказания в сервис,

который берет на вход данные с сайта sberautopodpiska.ru и отдает на

выход предсказание - совершит пользователь целевое действие или

нет

Выполнена: **Модель упакована в сервис, который берет на вход**

**данные из внешних источников и отдает на выход предсказание -**

**совершит пользователь целевое действие или нет**

